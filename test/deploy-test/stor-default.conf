########################### Common ###########################
[COMMON]
    user = root
    install_path = /usr/local
    ssh_port = 22
    env:JAVA_HOME = /usr/java/default

####################### Zookeeper Common #####################
[ZK_COMMON]
    mounts =
    mkfs_cmd = mkfs.xfs -f -i size=1024
    mount_opts  = rw,noatime,seclabel,attr2,inode64,logbsize=256k,noquota
    package =

    #these configs are for ${ZK_HOME}/conf/zoo.cfg
    cfg:tickTime = 2000  #prefix "cfg:" stands for file "zoo.cfg"
    cfg:initLimit=10
    cfg:syncLimit=5
    cfg:dataDir=
    cfg:dataLogDir=
    cfg:clientPort=2181
    cfg:maxClientCnxns=300

    #these configs are for ${ZK_HOME}/bin/zkEnv.sh
    env:ZOOPIDFILE =
    env:ZOO_LOG_DIR=
    env:ZOO_LOG4J_PROP= INFO,ROLLINGFILE

########################## HDFS Common #######################
[HDFS_COMMON]
    mounts =
    mkfs_cmd = mkfs.ext4
    mount_opts = rw,noatime,seclabel,data=ordered
    package =

    #these configs are for ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
    env:HADOOP_PID_DIR=
    env:HADOOP_LOG_DIR=
    env:HADOOP_OPTS = -Xmx4096m $HADOOP_OPTS
    env:HADOOP_CLIENT_OPTS = -Xmx2048m $HADOOP_CLIENT_OPTS

    #these configs are for ${HADOOP_HOME}/etc/hadoop/core-site.xml
    core-site:fs.defaultFS = hdfs://mycluster
    core-site:io.file.buffer.size = 131072
    core-site:io.compression.codecs = org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.SnappyCodec
    core-site:hadoop.tmp.dir=

    #these configs are for ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml
    hdfs-site:dfs.hosts.exclude=
    hdfs-site:dfs.nameservices = mycluster
    hdfs-site:dfs.ha.namenodes.mycluster = nn1,nn2
    hdfs-site:dfs.namenode.rpc-address.mycluster.nn1 = 0.0.0.0:8020
    hdfs-site:dfs.namenode.rpc-address.mycluster.nn2 = 0.0.0.0:8020
    hdfs-site:dfs.namenode.servicerpc-address.mycluster.nn1 = 0.0.0.0:8040
    hdfs-site:dfs.namenode.servicerpc-address.mycluster.nn2 = 0.0.0.0:8040
    hdfs-site:dfs.namenode.lifeline.rpc-address.mycluster.nn1 = 0.0.0.0:8050
    hdfs-site:dfs.namenode.lifeline.rpc-address.mycluster.nn2 = 0.0.0.0:8050
    hdfs-site:dfs.namenode.http-address.mycluster.nn1 = 0.0.0.0:50070
    hdfs-site:dfs.namenode.http-address.mycluster.nn2 = 0.0.0.0:50070
    hdfs-site:dfs.namenode.shared.edits.dir =
    hdfs-site:dfs.client.failover.proxy.provider.mycluster = org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
    hdfs-site:dfs.ha.fencing.methods = shell(/bin/true)
    hdfs-site:dfs.ha.automatic-failover.enabled = true
    hdfs-site:ha.zookeeper.quorum =
    hdfs-site:dfs.replication = 3
    hdfs-site:dfs.blocksize = 268435456
    hdfs-site:dfs.journalnode.edits.dir =
    hdfs-site:dfs.namenode.name.dir =
    hdfs-site:dfs.datanode.data.dir =
    hdfs-site:dfs.namenode.handler.count=64
    hdfs-site:dfs.datanode.handler.count=10
    hdfs-site:dfs.namenode.avoid.read.stale.datanode=true
    hdfs-site:dfs.namenode.avoid.write.stale.datanode=true
    hdfs-site:dfs.namenode.stale.datanode.interval=30000
    hdfs-site:dfs.namenode.check.stale.datanode=true
    hdfs-site:dfs.namenode.heartbeat.recheck-interval = 300000
    hdfs-site:dfs.heartbeat.interval = 3
    hdfs-site:dfs.client.read.shortcircuit = true
    hdfs-site:dfs.datanode.failed.volumes.tolerated = 0
    hdfs-site:dfs.datanode.sync.behind.writes = true
    hdfs-site:dfs.domain.socket.path =
    hdfs-site:dfs.client.file-block-storage-locations.timeout = 3000
    hdfs-site:dfs.datanode.max.transfer.threads = 4096

[HBASE_COMMON]
    package =

    #these configs are for ${HBASE_HOME}/conf/hbase-env.sh
    env:HBASE_HEAPSIZE = 10G
    env:HBASE_OFFHEAPSIZE = 8G
    env:HBASE_OPTS=-Xmx10g -Xms10g -Xmn512m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/var/hbase/logs/gc-$(hostname)-hbase.log -Djava.library.path=/usr/local/hadoop-2.8.0/lib/native
    env:HBASE_LOG_DIR = /var/hbase/logs
    env:HBASE_PID_DIR = /var/hbase/pid
    env:HBASE_MANAGES_ZK = false

    #these configs are for ${HBASE_HOME}/conf/hbase-site.xml
    hbase-site:hbase.zookeeper.useMulti = true
    hbase-site:hbase.rootdir = hdfs://mycluster/hbase
    hbase-site:hbase.cluster.distributed = true
    hbase-site:hbase.zookeeper.quorum = 
    hbase-site:zookeeper.session.timeout = 180000
    hbase-site:hbase.regionserver.checksum.verify = true
    hbase-site:hbase.hstore.compactionThreshold = 3
    hbase-site:hbase.hstore.blockingStoreFiles = 10
    hbase-site:hbase.hregion.memstore.flush.size = 268435456 #256M
    hbase-site:hbase.hregion.memstore.block.multiplier = 4
    hbase-site:hbase.regionserver.global.memstore.size = 0.4
    hbase-site:hbase.regionserver.global.memstore.size.lower.limit = 0.6
    hbase-site:hfile.block.cache.size = 0.4
    hbase-site:hbase.hregion.majorcompaction = 604800000 
    hbase-site:hbase.hregion.majorcompaction.jitter = 0.50 
    hbase-site:hbase.hregion.max.filesize = 21474836480         #20G
    hbase-site:hbase.client.write.buffer = 2097152              #2M 
    hbase-site:hbase.client.scanner.max.result.size = 2097152   #2M
    hbase-site:hbase.server.scanner.max.result.size = 104857600 #100M
    hbase-site:hbase.regionserver.handler.count = 120
    hbase-site:hbase.dfs.client.read.shortcircuit.buffer.size = 131072  #128K
    hbase-site:hbase.client.pause = 100
    hbase-site:hbase.client.retries.number = 35
    hbase-site:hbase.regionserver.thrift.framed = true
    hbase-site:hbase.regionserver.thrift.framed.max_frame_size_in_mb = 2
    hbase-site:hbase.ipc.client.tcpnodelay = true
    hbase-site:dfs.client.hedged.read.threadpool.size = 20
    hbase-site:dfs.client.hedged.read.threshold.millis = 10
    hbase-site:hbase.master.port = 16000
    hbase-site:hbase.master.info.port = 16010
    hbase-site:hbase.master.info.bindAddress = 0.0.0.0
