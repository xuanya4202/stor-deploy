########################### Common ###########################
[COMMON]
    user = root
    install_path = /usr/local
    ssh_port = 22
    env:JAVA_HOME = /usr/java/default

####################### Zookeeper Common #####################
[ZK_COMMON]
    mounts =
    mkfs_cmd = mkfs.xfs -f -i size=1024
    mount_opts  = rw,noatime,seclabel,attr2,inode64,logbsize=256k,noquota
    package =

    #these configs are for ${ZK_HOME}/conf/zoo.cfg
    cfg:tickTime = 2000  #prefix "cfg:" stands for file "zoo.cfg"
    cfg:initLimit=10
    cfg:syncLimit=5
    cfg:dataDir=
    cfg:dataLogDir=
    cfg:clientPort=2181
    cfg:maxClientCnxns=300

    #these configs are for ${ZK_HOME}/bin/zkEnv.sh
    env:ZOOPIDFILE =
    env:ZOO_LOG_DIR=
    env:ZOO_LOG4J_PROP= INFO,ROLLINGFILE

########################## HDFS Common #######################
[HDFS_COMMON]
    mounts =
    mkfs_cmd = mkfs.ext4
    mount_opts = rw,noatime,seclabel,data=ordered
    package =

    #these configs are for ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
    env:HADOOP_PID_DIR=
    env:HADOOP_LOG_DIR=
    env:HADOOP_OPTS = -Xmx4096m $HADOOP_OPTS
    env:HADOOP_CLIENT_OPTS = -Xmx2048m $HADOOP_CLIENT_OPTS

    #these configs are for ${HADOOP_HOME}/etc/hadoop/core-site.xml
    core-site:fs.defaultFS=hdfs://stor
    core-site:io.file.buffer.size = 131072
    core-site:io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.SnappyCodec
    core-site:hadoop.tmp.dir=

    #these configs are for ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml
    hdfs-site:dfs.hosts.exclude=
    hdfs-site:dfs.nameservices = stor
    hdfs-site:dfs.ha.namenodes.stor = nn1,nn2
    hdfs-site:dfs.namenode.rpc-address.stor.nn1 =
    hdfs-site:dfs.namenode.rpc-address.stor.nn2 =
    hdfs-site:dfs.namenode.http-address.stor.nn1 = 0.0.0.0:50070
    hdfs-site:dfs.namenode.http-address.stor.nn2 = 0.0.0.0:50070
    hdfs-site:dfs.namenode.shared.edits.dir =
    hdfs-site:dfs.client.failover.proxy.provider.stor = org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
    hdfs-site:dfs.ha.fencing.methods = shell(/bin/true)
    hdfs-site:dfs.ha.automatic-failover.enabled = true
    hdfs-site:ha.zookeeper.quorum =
    hdfs-site:dfs.replication = 3
    hdfs-site:dfs.blocksize = 268435456
    hdfs-site:dfs.journalnode.edits.dir =
    hdfs-site:dfs.namenode.name.dir =
    hdfs-site:dfs.datanode.data.dir =
    hdfs-site:dfs.namenode.handler.count=64
    hdfs-site:dfs.datanode.handler.count=8
    hdfs-site:dfs.namenode.avoid.read.stale.datanode=true
    hdfs-site:dfs.namenode.avoid.write.stale.datanode=true
    hdfs-site:dfs.namenode.stale.datanode.interval=30000
    hdfs-site:dfs.namenode.check.stale.datanode=true
    hdfs-site:dfs.namenode.heartbeat.recheck-interval = 300000
    hdfs-site:dfs.heartbeat.interval = 3
    hdfs-site:dfs.client.read.shortcircuit = true
    hdfs-site:dfs.datanode.failed.volumes.tolerated = 0
    hdfs-site:dfs.datanode.sync.behind.writes = true
    hdfs-site:dfs.domain.socket.path =
    hdfs-site:dfs.client.file-block-storage-locations.timeout = 3000
    hdfs-site:dfs.datanode.max.transfer.threads = 4096
